{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-18T08:33:38.907953Z","iopub.execute_input":"2023-07-18T08:33:38.908526Z","iopub.status.idle":"2023-07-18T08:33:38.917490Z","shell.execute_reply.started":"2023-07-18T08:33:38.908467Z","shell.execute_reply":"2023-07-18T08:33:38.915725Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Digit Recognizer\n## Competition Description\n\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n\nIn this competition, The goal is to correctly identify digits from a dataset of tens of thousands of handwritten images.","metadata":{}},{"cell_type":"markdown","source":"### Dataset Description:\n    \nThe data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n\nThe training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n\nEach pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n\nFor example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n\nVisually, if we omit the \"pixel\" prefix, the pixels make up the image like this:\n\n000 001 002 003 ... 026 027\n\n028 029 030 031 ... 054 055\n\n056 057 058 059 ... 082 083\n\n |   |   |   |  ...  |   |\n \n728 729 730 731 ... 754 755\n\n756 757 758 759 ... 782 783\n\n\nThe test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n\nYour submission file should be in the following format: For each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict. For example, if you predict that the first image is of a 3, the second image is of a 7, and the third image is of a 8, then your submission file would look like:\n\nImageId,Label\n\n1,3\n\n2,7\n\n3,8 \n\n(27997 more lines)\n\nThe evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images.","metadata":{}},{"cell_type":"markdown","source":"Step 1: Data Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the training and test datasets\ntrain_data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest_data = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T08:33:38.920328Z","iopub.execute_input":"2023-07-18T08:33:38.920749Z","iopub.status.idle":"2023-07-18T08:33:43.031457Z","shell.execute_reply.started":"2023-07-18T08:33:38.920706Z","shell.execute_reply":"2023-07-18T08:33:43.029849Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Split features (pixel values) and labels (digit values) in the training data\ntrain_features = train_data.drop('label', axis=1)\ntrain_labels = train_data['label']","metadata":{"execution":{"iopub.status.busy":"2023-07-18T08:33:43.033316Z","iopub.execute_input":"2023-07-18T08:33:43.033848Z","iopub.status.idle":"2023-07-18T08:33:43.132719Z","shell.execute_reply.started":"2023-07-18T08:33:43.033796Z","shell.execute_reply":"2023-07-18T08:33:43.131339Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Preprocessing: Scale pixel values between 0 and 1\ntrain_features = train_features / 255.0\ntest_data = test_data / 255.0","metadata":{"execution":{"iopub.status.busy":"2023-07-18T08:33:43.134028Z","iopub.execute_input":"2023-07-18T08:33:43.134481Z","iopub.status.idle":"2023-07-18T08:33:43.245760Z","shell.execute_reply.started":"2023-07-18T08:33:43.134438Z","shell.execute_reply":"2023-07-18T08:33:43.244888Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Convert labels to one-hot encoded vectors\nnum_classes = 10  # Digits from 0 to 9\ntrain_labels = pd.get_dummies(train_labels).values","metadata":{"execution":{"iopub.status.busy":"2023-07-18T08:33:43.248033Z","iopub.execute_input":"2023-07-18T08:33:43.248398Z","iopub.status.idle":"2023-07-18T08:33:43.257269Z","shell.execute_reply.started":"2023-07-18T08:33:43.248364Z","shell.execute_reply":"2023-07-18T08:33:43.256316Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Split the training data into training and validation sets\ntrain_features, validation_features, train_labels, validation_labels = train_test_split(train_features, train_labels, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-18T08:33:43.258990Z","iopub.execute_input":"2023-07-18T08:33:43.260095Z","iopub.status.idle":"2023-07-18T08:33:43.493162Z","shell.execute_reply.started":"2023-07-18T08:33:43.260055Z","shell.execute_reply":"2023-07-18T08:33:43.491915Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"Step 2: Model Selection\n\nFor this task, let's use a Convolutional Neural Network (CNN), which is a popular choice for image classification tasks due to their ability to capture spatial patterns in images.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# Build the CNN model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T08:33:43.494659Z","iopub.execute_input":"2023-07-18T08:33:43.495046Z","iopub.status.idle":"2023-07-18T08:33:43.566046Z","shell.execute_reply.started":"2023-07-18T08:33:43.495011Z","shell.execute_reply":"2023-07-18T08:33:43.564784Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"Step 3: Model Training","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\n# Compile the model\nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(train_features.values.reshape(-1, 28, 28, 1), train_labels, batch_size=128, epochs=10, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T08:33:43.567795Z","iopub.execute_input":"2023-07-18T08:33:43.568239Z","iopub.status.idle":"2023-07-18T08:35:06.539946Z","shell.execute_reply.started":"2023-07-18T08:33:43.568202Z","shell.execute_reply":"2023-07-18T08:35:06.539009Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Epoch 1/10\n210/210 [==============================] - 10s 42ms/step - loss: 0.4818 - accuracy: 0.8525 - val_loss: 0.1706 - val_accuracy: 0.9521\nEpoch 2/10\n210/210 [==============================] - 8s 36ms/step - loss: 0.2000 - accuracy: 0.9410 - val_loss: 0.1080 - val_accuracy: 0.9699\nEpoch 3/10\n210/210 [==============================] - 8s 36ms/step - loss: 0.1353 - accuracy: 0.9589 - val_loss: 0.0891 - val_accuracy: 0.9760\nEpoch 4/10\n210/210 [==============================] - 7s 35ms/step - loss: 0.1046 - accuracy: 0.9674 - val_loss: 0.0770 - val_accuracy: 0.9789\nEpoch 5/10\n210/210 [==============================] - 8s 37ms/step - loss: 0.0861 - accuracy: 0.9729 - val_loss: 0.0694 - val_accuracy: 0.9817\nEpoch 6/10\n210/210 [==============================] - 7s 35ms/step - loss: 0.0754 - accuracy: 0.9775 - val_loss: 0.0651 - val_accuracy: 0.9815\nEpoch 7/10\n210/210 [==============================] - 7s 35ms/step - loss: 0.0675 - accuracy: 0.9799 - val_loss: 0.0630 - val_accuracy: 0.9826\nEpoch 8/10\n210/210 [==============================] - 7s 35ms/step - loss: 0.0573 - accuracy: 0.9820 - val_loss: 0.0650 - val_accuracy: 0.9824\nEpoch 9/10\n210/210 [==============================] - 8s 36ms/step - loss: 0.0507 - accuracy: 0.9838 - val_loss: 0.0640 - val_accuracy: 0.9820\nEpoch 10/10\n210/210 [==============================] - 7s 35ms/step - loss: 0.0468 - accuracy: 0.9851 - val_loss: 0.0622 - val_accuracy: 0.9839\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7a98e1806470>"},"metadata":{}}]},{"cell_type":"markdown","source":"Step 4: Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the validation set\nval_loss, val_acc = model.evaluate(validation_features.values.reshape(-1, 28, 28, 1), validation_labels)\n\nprint('Validation Loss:', val_loss)\nprint('Validation Accuracy:', val_acc)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T08:35:06.540918Z","iopub.execute_input":"2023-07-18T08:35:06.541896Z","iopub.status.idle":"2023-07-18T08:35:07.976781Z","shell.execute_reply.started":"2023-07-18T08:35:06.541858Z","shell.execute_reply":"2023-07-18T08:35:07.975047Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"263/263 [==============================] - 1s 5ms/step - loss: 0.0530 - accuracy: 0.9848\nValidation Loss: 0.0529676117002964\nValidation Accuracy: 0.9847618937492371\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Step 5: Model Prediction","metadata":{}},{"cell_type":"code","source":"# Make predictions on the test data\ntest_predictions = model.predict(test_data.values.reshape(-1, 28, 28, 1))\n\n# Convert predictions to labels\ntest_labels = test_predictions.argmax(axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T08:35:07.978625Z","iopub.execute_input":"2023-07-18T08:35:07.979057Z","iopub.status.idle":"2023-07-18T08:35:13.082253Z","shell.execute_reply.started":"2023-07-18T08:35:07.979022Z","shell.execute_reply":"2023-07-18T08:35:13.080348Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"875/875 [==============================] - 4s 5ms/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Step 6: Submission File Generation","metadata":{}},{"cell_type":"code","source":"# Generate submission file\nsubmission = pd.DataFrame({'ImageId': range(1, len(test_labels) + 1), 'Label': test_labels})\n\n# Save submission file\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T08:35:13.083957Z","iopub.execute_input":"2023-07-18T08:35:13.084684Z","iopub.status.idle":"2023-07-18T08:35:13.132053Z","shell.execute_reply.started":"2023-07-18T08:35:13.084632Z","shell.execute_reply":"2023-07-18T08:35:13.130876Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}